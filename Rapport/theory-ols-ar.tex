%!TEX root=report.tex

\subsection{OLS with autocorrelated residuals}

In OLS it is assumed that the residuals between different observations are uncorrelated - i.e. $E[\epsilon_i \epsilon_j | X]=0, \forall i \neq j$. 
However, since our data is both time series and spatial this assumption most likely does not hold (this is quantified with the Durbin-Watson test statistic in the results section). 
The violation of this assumption can lead to an underestimation of the variance of the residuals $\sigma_e$\footnote{www.en.wikipedia.org/wiki/Autocorrelation}.
 This in turn leads to an overestimation of the $\beta$ parameters' t-scores and thus the p-values will be erroneous. 
\\
To correct for the above one can assume that the error terms are created by a stochastic process. 
One can imagine that this process can receive exogenous influences or shocks $u_i$ at every point in time $t_i$ and that these shocks will propagate through time in such a way that their "ripple" effects slowly subside.
By assuming that $u_i$ is a stochastic variable with mean $E[u_i]=0$ and variance $var[u_i]=\sigma_u^2$, the residuals from the OLS model becomes a weighted sum of absorbed shocks 
\begin{equation}
\epsilon_i=u_i+\alpha_1 u_{i-1}+\alpha_2 u_{i-2}+...
\end{equation}

 The $u_i$s are assumed to be independent and identically distributed (i.i.d.).
By letting $\alpha_i=\rho^i$ and assuming $|\rho|<1$ one achieves the formula for a first order Autoregressive process (denoted AR(1))
\begin{equation}
\epsilon_i=\rho \epsilon_{i-1}+u_i
\end{equation}

In the above it is apparent that $\rho$ is the correlation between any given residual and its successor. 
Thus the OLS normal equation can be rewritten as
\begin{equation}\label{olsAR}
\text{MIN}_{\beta,\rho} (Y-X\beta)^T \Sigma^{-1}(Y-X\beta)
\end{equation}
 where $ \Sigma^{-1}$ is given as
\begin{equation}
\begin{bmatrix}
1 & -\rho & 0 & \cdots & 0 & 0\\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0\\
0 & -\rho & 1+\rho^2 & -\rho &\cdots & 0  \\
\vdots &\ddots & \ddots & \ddots & \ddots & 0 \\
0 & 0 &0& -\rho & 1+\rho^2 & -\rho  \\
0 & 0 &0&0&-\rho & 1
\end{bmatrix}
\end{equation}

Since equation system \ref{olsAR} is nonlinear there is no closed form solution to this problem. However, a practical way of solving the problem looks as follows:
\begin{enumerate}
\item initialize by letting $\rho=0$ and estimate $\beta$ with the usual OLS method.
\item Iterate: \begin{enumerate}
\item Keep $\beta$ constant and estimate $\rho$ by $\hat{\rho}=\sum_{i=2}^n \hat{\epsilon_i}\hat{\epsilon_{i-1}}/ \sum_{i=2}^{n-1} \hat{\epsilon_i}^2$
\item Keep $\rho$ constant and assume  $\rho=\hat{\rho}$. Estimate $\beta$ via \ref{olsAR}
\item Repeat until convergence or some predetermined upper iteration boundary is met.
\end{enumerate}
\end{enumerate}
