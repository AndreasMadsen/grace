%!TEX root=report.tex
\subsection{KMeans Clustering}
There are multiple names for the process of grouping or dividing a set of objects into disjunct subset which from now on will be referred to as "clustering". The goal is that the objects in each cluster should be similar.
 Clustering is unsupervised learning, because there is no "correct" answer. 
Given a set of objects even humans will differ in their choice of clusters and the same is the case in machine learning.
 Alas in this section only one method will be shown and argued for.

Since similarity is subjective one needs to define what distance function to use as the Dissimilarity Measure $D$. 
The are only 2 conditions a function $d$ must satisfy to be a valid distance metric, namely
\begin{equation}
d(p_1,p_2)\le d(p_1,p_3)+d(p_3,p_2) \text{   }\forall p_1,p_2,p_3
\end{equation} and that
\begin{equation}
d(x,y)=d(y,x)  \text{   } \forall x,y
\end{equation}.
In this report the squared Euclidean distance has been chosen as the dissimilarity measure.
 This because the dimension in the input space are all normal distributed and with approximately similar probability density functions. Let $x_k$ be  the k'th row in the design matrix $X$; we then have 
\begin{equation}
D(x_i,x_j)=(x_i-x_j)(x_i-x_j)^{T}
\end{equation}.
This gives rises to a measure of the total cluster variance $C^{*}$ which for any given clustering can be calculated as
\begin{equation}
C^{*}=\sum^K_{k=1} N_k \sum_{C(i)=k} D(x_i-\mu_k)
\end{equation}
where $\mu_k$ denotes the k'th cluster center (also called a centroid), $N_k$ the number of observations in cluster $k$ and C(i)=k denotes all the points $i$ in the cluster $k$.  

Having decided on a distance measure, one now needs to choose a clustering technique.
 Due to the size of $X$,$ (lat*lon,days)=(64800,341)$, a robust and scaleable method is needed. 
Scalable both as in the sheer size of X, but also as in the number of clusters needed to capture the distributions of the data.
 For these reasons, KMeans was chosen. 
\\
The assumption of KMeans is that the dimensions in the data have almost the same scale and that the clusters covariance matrix is a diagonal matrix (i.e. doesn't account for cross correlation between the different days). 
The KMeans algorithm is iterative and can be described in 4 simple steps.

\begin{enumerate}
	\item Initialize cluster centroids
	\item Iterate until centroid convergence:
		\begin{enumerate}
			\item for a given cluster assigment C, minimize C* by recomputing the centroids for each cluster as the mean of points in cluster
			\item Given the current set of centroids reassign each observation to the closest centroid
		\end{enumerate}
\end{enumerate}

So how does one know how many clusters are needed? Hastie, Tibshirani and Firedman proposes calculating gap-statistics \cite[p.~519]{statistical-learning}.
For a given clustering first one needs to calculate and sum the intra cluster distances;
\begin{equation}
D_k=\sum_{x_i\in C_k} \sum_{x_j\in C_k} d(x_i,x_j)
\end{equation}.
Then follows a weighting and sum over all the clusters
\begin{equation}
W_{K}=\sum_{k=1}^K \frac{1}{2n_k} D_k
\end{equation}
where $K$ is the number of clusters and $n_k$ the observations in the k'th cluster.
Given the quantity $W_k$ one can then calculate a gap by simulating $b$ datasets $B$ drawn from a random uniform distribution $X$ and calculating the gap as
\begin{equation}
Gap(k)=E^{*}\{log W_k\} - log(W_k)
\end{equation}
where the expected value $E^{*}$ can be estimated by the mean over the simulated datasets $B$. That is the gap statistics tries to avoid overfitting by comparing the cluster gain on a dataset where "there are no clusters" (random uniform).
After calculating the standard deviation of $W_k$ on $B$ as 
\begin{equation}
\sigma_k=\sigma{W_k | B}\sqrt{1+\frac{1}{b}}
\end{equation}
 one can get the optimal number of clusters $k$ by finding the minimum k that satisfies
\begin{equation}
gap(k)\ge gap(k+1)-s_{k+1}
\end{equation}