%!TEX root=report.tex
\subsection{Gaussian Mixture Model (GMM)}
The GMM is a more advanced clustering model than K-Means. 
The main advantage is that GMM allows for hyper elliptical clusters. This uses gaussian kernels with its the shape described in a covariance matrix. A result similar to K-means could be obtained by forcing this covariance matrix to be the identity matrix.

Restrictions on the covariance matrix (i.e. shared covariance, diagonal covariance, spherical covariance etc.) can easily be applied in GMM and is quite common. In this analysis however, no covariance restrictions will be used.

In the GMM the assumption is that data comes from a single density function.
The density function is assumed to be a combination (mixture) of $K$ Gaussian PDFs where K is finite and denotes the number of mixture components (i.e. clusters).

Each mixture component has a centroid (the mean), a covariance matrix and a mixing weight.
The sum of mixing weights across components has to be one for the GMM to constitute an actual pdf.

To estimate the model parameters several different methods exists. The most common and is the expectation maximization (EM) algorithm which is quite complex and thus wont be described here. To the curious reader we recommend \cite[p.~214,272,463]{statistical-learning}).

In practice if K is large and the vector space $X$ is high dimensional, estimation of the model parameters will take too much computing power, and even if one get model parameters to converge, the degrees of freedom will be low.
In our case the input space would be 341-dimensional (341 observations in time per location) and thus a dimensionality reduction of some kind is needed. 

\subsubsection{Dimensionality reduction}
In many cases when dealing with high dimensional data, most of the data lies on a lower dimensional manifold. 
Different methods exists to try to identify such manifolds, but in this analysis the previously described technique PCA will be used. \todo{Now we use kernel PCA, not just linear PCA}
This is done by selecting only the most important principal components from the PCA, thus forcing the data onto a lower dimensional manifold.
The GRACE data contains quite a bit of noise and one might hope that the noise will be primarily contained in its own principal components. Hopefully those PCs will only account for a small amount of the variance in the data. 
Thus when selecting only the most significance PCs some of the noise will be "lost".
This combined with the more flexible GMM over K-Means, will hopefully lead to better clustering.
